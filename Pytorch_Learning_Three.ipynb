{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Learning_Three.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPot0snic7E08luz8m0a12K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs17emds11029/googlecolab/blob/master/Pytorch_Learning_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cydqmg4-nria",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we shall set up a simple nueral network with the help of torch.nn package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DalysMTmnkHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sox-yS5Xn_Tj",
        "colab_type": "text"
      },
      "source": [
        "We shall use the tutorial from pytorch.org for image classification across 10 classes. The 10 classes can be the 10 digits in the MNIST dataset. For now we shall use the network with a random input image of a chosen size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXHrHpe1n-Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6EzvUI1pbIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "  # All new networks start with subclassing the nn.Module class where a Module is defined as a logical grouping of ANN layers\n",
        "  def __init__(self):\n",
        "    super(LeNet,self).__init__() # Call the super classe's __init__ method\n",
        "    # In the __init__ method we define the layers of the network with required sizes\n",
        "    self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "    # This is the understanding of a convolution operation. The first parameter is the number of channels\n",
        "    # parameter one for Grey scale images is 1 and for RGB images is 3\n",
        "    # parameter 2 is the number of output channels, which can be chosen according to a network architecture\n",
        "    # paramter 2 here is 6, so there are 6 output channels, and 6 kerners would operate on the input channel\n",
        "    # parameter 3 is the kernel size, which is a 3x3 square kernel with stride one.\n",
        "    # we did not define the height and width of the image here\n",
        "    self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "    # from the output of this layer we have 16 channels of the image with different convolution filters\n",
        "    # each one of these 16 channels would identify a particular aspect of the image\n",
        "    self.fc1 = nn.Linear(16*6*6, 120)\n",
        "    # here for the fully connected layer we are defining the image size in our 16 channels as 6x6\n",
        "    # how did we get the image as 6x6? We shall verify with the math\n",
        "    # the number of output nodes as 120 which can be chosen according to the architecture\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "    # we conclude the network with 10 output nodes for 10 class classification\n",
        "  \n",
        "  def forward(self,x):\n",
        "    # Now we have to fill the architecture my making the connection between each layer\n",
        "    # We do this in the forward pass of the network and the computation framework of torch.nn.autograd would \n",
        "    # automatically handle the parameter update by back propogation\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
        "    # Here we first passed the input image that is X to the conv1 layer, applied an activation to its ouput\n",
        "    # and then applied a Max Pooling operation with a 2x2 square kernel\n",
        "    # now X has the ouput from the Conv-Relu-MaxPool block that is the first with 6 channels\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)),(2,2))\n",
        "    # now X has 16 channels and the dimension of x tensor is 16*h*w\n",
        "    x = x.view(-1, self.num_flat_features(x))\n",
        "    # We are flattening the tensor to a vector here\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "  \n",
        "  def num_flat_features(self,x):\n",
        "    size = x.size()[1:]\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oQZknCpxD8s",
        "colab_type": "text"
      },
      "source": [
        "In the above cell, we defined lot of things. We defined the arch. of out Nueral Network, we defined the forward function (mandatory) of how the input (image) should progress through the network. It is possible that we can write display for the intermediate representations of the image in the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yTGG3OZxhg2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "131cf461-79ab-4c85-a59c-320f219b3b81"
      },
      "source": [
        "# Lets initialize the netowk\n",
        "lenet = LeNet()\n",
        "print(lenet)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KL_mOT4_hzp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "4a558928-deb3-47b8-93cd-9c48990d8a67"
      },
      "source": [
        "params = list(lenet.parameters())\n",
        "for param in params:\n",
        "  print(param.size())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6, 1, 3, 3])\n",
            "torch.Size([6])\n",
            "torch.Size([16, 6, 3, 3])\n",
            "torch.Size([16])\n",
            "torch.Size([120, 576])\n",
            "torch.Size([120])\n",
            "torch.Size([84, 120])\n",
            "torch.Size([84])\n",
            "torch.Size([10, 84])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JozQwlndE8_G",
        "colab_type": "text"
      },
      "source": [
        "Lets understand the parameter tensors above. We know that conv1 takes 1 channel as input and 6 channels as output with a 3x3 kernel. So there are in effect 1 * 6 * 3 * 3 kernel cells or paramters for conv1. Hence the size of conv1 parameter tensor is 6,1,3,3. The next tensor of size 6 is the weight's on the edges from the 6 output channels to the next convolution block. These are not kernel cells. Similarly one can understand the parameters until the output nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28gUrtQgGIES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d4574f5e-46dd-4499-f788-ee4ffdf5ecdc"
      },
      "source": [
        "input = torch.randn(1,1,32,32) #batch size is one, channel is one, and image res is 32x32\n",
        "# after convolution with a 3x3 kernel the image rep. becomes 30x30, and with 2x2 max pool the image res. becomes 28x28\n",
        "out = lenet(input)\n",
        "print(out)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0087,  0.0871,  0.1658, -0.0222,  0.0193,  0.0642,  0.1592,  0.0725,\n",
            "         -0.0765, -0.0130]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y7FcckaL9jx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lenet.zero_grad()\n",
        "out.backward(torch.randn(1,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twQwI-4cMiqs",
        "colab_type": "text"
      },
      "source": [
        "In the above step, we set the gradients of the network to zero and done a backward stop that does automatic differentiation with respect to the output. The loss is output itself. In the next code blocks, we shall compute the actual loss with respect to a dummy target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct6g-lYhNXaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7d2e3026-24c7-4e7b-d953-2fa84aa6743e"
      },
      "source": [
        "output = lenet(input)\n",
        "target = torch.randn(10)\n",
        "target = target.view(1,-1)\n",
        "print(target)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.1712,  0.0261, -0.0733,  1.6994,  0.1407, -0.3993,  0.9905,  0.8395,\n",
            "         -0.0738, -1.7455]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF1_0Sv4Nyk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2fb6a29-2adc-4c79-a078-0fd71936917a"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8887, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9H6AxoDOK4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "dd6a5e18-ea8a-490f-be2c-b14a85b661bf"
      },
      "source": [
        "# Now we use this loss to call the backward function to adjust the gradients\n",
        "# We can look at the loss propogation or computation graph starting from 'input'\n",
        "# tensor to the calculation of 'loss'.\n",
        "print(loss.grad_fn)\n",
        "print(loss.grad_fn.next_functions[0][0])\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7fc94c467320>\n",
            "<AddmmBackward object at 0x7fc94c46f278>\n",
            "<AccumulateGrad object at 0x7fc94c467320>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgzccqV0O6mx",
        "colab_type": "text"
      },
      "source": [
        "We can also integrate this with TensorBoard to view the computation or nueral network architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfXK961NPC_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets define the loop that will calculate the loss for each input chunk in the mini batch\n",
        "# and update the parameters\n",
        "lenet.zero_grad()\n",
        "loss.backward()\n",
        "learning_rate = 0.01\n",
        "for f in lenet.parameters():\n",
        "  f.data.sub_(learning_rate*f.grad.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3QPm4mAPqA7",
        "colab_type": "text"
      },
      "source": [
        "The above step is default update rule of the parameters, i.e.\n",
        "new_value = old_value - learining_rate * gradient. However the nice people has created an 'optim' or optimizer package at torch.optim that has various versions of parameter update like Adam, Nesterov etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljw2cwfDQif3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# use stochastic gradient descent\n",
        "optimizer = optim.SGD(lenet.parameters(), lr=0.01)\n",
        "optimizer.zero_grad() # the net is now encapsulated in the optimizer\n",
        "# Setting the optimizer gradients to zero would set the gradients on the underlying \n",
        "# neural network to zero. This would enable us to compare multiple optimizers without instantiating\n",
        "# too many objects of LeNet\n",
        "output = lenet(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVDfhaSORh0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e4682328-7e12-4746-b7b6-3040d75b7eb1"
      },
      "source": [
        "print(params[9])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([ 0.0066,  0.0910,  0.0441, -0.0424,  0.0419,  0.0550,  0.0838,  0.0952,\n",
            "        -0.1043, -0.0216], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd1X6vJRRj5T",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}